{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS3--Boosting and Bagging Models with Hyperparameter Tuning\n",
    "\n",
    "Richard Yang"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                 1       2           3   4                   5   \\\n",
       "0      39         State-gov   77516   Bachelors  13       Never-married   \n",
       "1      50  Self-emp-not-inc   83311   Bachelors  13  Married-civ-spouse   \n",
       "2      38           Private  215646     HS-grad   9            Divorced   \n",
       "3      53           Private  234721        11th   7  Married-civ-spouse   \n",
       "4      28           Private  338409   Bachelors  13  Married-civ-spouse   \n",
       "...    ..               ...     ...         ...  ..                 ...   \n",
       "32556  27           Private  257302  Assoc-acdm  12  Married-civ-spouse   \n",
       "32557  40           Private  154374     HS-grad   9  Married-civ-spouse   \n",
       "32558  58           Private  151910     HS-grad   9             Widowed   \n",
       "32559  22           Private  201490     HS-grad   9       Never-married   \n",
       "32560  52      Self-emp-inc  287927     HS-grad   9  Married-civ-spouse   \n",
       "\n",
       "                      6              7      8       9      10  11  12  \\\n",
       "0           Adm-clerical  Not-in-family  White    Male   2174   0  40   \n",
       "1        Exec-managerial        Husband  White    Male      0   0  13   \n",
       "2      Handlers-cleaners  Not-in-family  White    Male      0   0  40   \n",
       "3      Handlers-cleaners        Husband  Black    Male      0   0  40   \n",
       "4         Prof-specialty           Wife  Black  Female      0   0  40   \n",
       "...                  ...            ...    ...     ...    ...  ..  ..   \n",
       "32556       Tech-support           Wife  White  Female      0   0  38   \n",
       "32557  Machine-op-inspct        Husband  White    Male      0   0  40   \n",
       "32558       Adm-clerical      Unmarried  White  Female      0   0  40   \n",
       "32559       Adm-clerical      Own-child  White    Male      0   0  20   \n",
       "32560    Exec-managerial           Wife  White  Female  15024   0  40   \n",
       "\n",
       "                  13     14  \n",
       "0      United-States  <=50K  \n",
       "1      United-States  <=50K  \n",
       "2      United-States  <=50K  \n",
       "3      United-States  <=50K  \n",
       "4               Cuba  <=50K  \n",
       "...              ...    ...  \n",
       "32556  United-States  <=50K  \n",
       "32557  United-States   >50K  \n",
       "32558  United-States  <=50K  \n",
       "32559  United-States  <=50K  \n",
       "32560  United-States   >50K  \n",
       "\n",
       "[32561 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "adult_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header = None, skipinitialspace=True)\n",
    "adult_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  fnlwgt   education  education-num  \\\n",
       "0       39         State-gov   77516   Bachelors             13   \n",
       "1       50  Self-emp-not-inc   83311   Bachelors             13   \n",
       "2       38           Private  215646     HS-grad              9   \n",
       "3       53           Private  234721        11th              7   \n",
       "4       28           Private  338409   Bachelors             13   \n",
       "...    ...               ...     ...         ...            ...   \n",
       "32556   27           Private  257302  Assoc-acdm             12   \n",
       "32557   40           Private  154374     HS-grad              9   \n",
       "32558   58           Private  151910     HS-grad              9   \n",
       "32559   22           Private  201490     HS-grad              9   \n",
       "32560   52      Self-emp-inc  287927     HS-grad              9   \n",
       "\n",
       "           marital-status         occupation   relationship   race     sex  \\\n",
       "0           Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1      Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2                Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3      Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4      Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "...                   ...                ...            ...    ...     ...   \n",
       "32556  Married-civ-spouse       Tech-support           Wife  White  Female   \n",
       "32557  Married-civ-spouse  Machine-op-inspct        Husband  White    Male   \n",
       "32558             Widowed       Adm-clerical      Unmarried  White  Female   \n",
       "32559       Never-married       Adm-clerical      Own-child  White    Male   \n",
       "32560  Married-civ-spouse    Exec-managerial           Wife  White  Female   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week native-country salary  \n",
       "0              2174             0              40  United-States  <=50K  \n",
       "1                 0             0              13  United-States  <=50K  \n",
       "2                 0             0              40  United-States  <=50K  \n",
       "3                 0             0              40  United-States  <=50K  \n",
       "4                 0             0              40           Cuba  <=50K  \n",
       "...             ...           ...             ...            ...    ...  \n",
       "32556             0             0              38  United-States  <=50K  \n",
       "32557             0             0              40  United-States   >50K  \n",
       "32558             0             0              40  United-States  <=50K  \n",
       "32559             0             0              20  United-States  <=50K  \n",
       "32560         15024             0              40  United-States   >50K  \n",
       "\n",
       "[32561 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append the column names to the dataframe\n",
    "\n",
    "list_columns = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','salary']\n",
    "adult_df.columns = list_columns\n",
    "adult_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your dataframe shape to verify that you have the correct # of rows and columns.\n",
    "adult_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Drop the 3rd column from the data (it is referred to as \"fnlwgt\" on UCI's website and is not necessary in this homework)\n",
    "\n",
    "adult_df.drop('fnlwgt', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24720\n",
       "1     7841\n",
       "Name: salary, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f) Use the .replace() method to make the following changes to the \"salary\" column: \"<=50K\" should become 0. \">50K\" should become 1\n",
    "\n",
    "adult_df['salary'].replace({'<=50K':0, '>50K':1}, inplace=True)\n",
    "adult_df['salary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e-h\n",
    "\n",
    "# Create your X dataframe (just your predictors). It should include every feature except for the target variable which is \"salary\".\n",
    "\n",
    "X = adult_df.drop('salary', axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your y dataframe (just your target variable). It should only be \"salary\". The values should only be 0 and 1.\n",
    "# convert y to 0 and 1\n",
    "\n",
    "y = adult_df['salary']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 107)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoded = pd.get_dummies(X)\n",
    "X_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22792, 107)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train / test set using an 70/30 split. Verify that you have the same number of columns in your X_train and X_test.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train.shape   # 70% of 32561 = 22792\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest Classifier - Base Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6835,  620],\n",
       "       [ 870, 1444]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start by creating a simple Random Forest only using default parameters - this will let us compare Boosting methods to Random Forest in binary classification problems.\n",
    "\n",
    "# a) Use the RandomForestClassifier in sklearn. Fit your model on the training data.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "# b) Use the fitted model to predict on test data. Use the .predict_proba() and the .predict() methods to get predicted probabilities as well as predicted classes.\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "y_pred_proba = rf.predict_proba(X_test)\n",
    "\n",
    "# c) Calculate the confusion matrix and classification report (both are in sklearn.metrics).\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90      7455\n",
      "           1       0.70      0.62      0.66      2314\n",
      "\n",
      "    accuracy                           0.85      9769\n",
      "   macro avg       0.79      0.77      0.78      9769\n",
      "weighted avg       0.84      0.85      0.84      9769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8897333004074577"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d) Calculate the AUC score.\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.227371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours-per-week</th>\n",
       "      <td>0.112285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital-gain</th>\n",
       "      <td>0.102617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education-num</th>\n",
       "      <td>0.066930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital-status_Married-civ-spouse</th>\n",
       "      <td>0.052850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   importance\n",
       "age                                  0.227371\n",
       "hours-per-week                       0.112285\n",
       "capital-gain                         0.102617\n",
       "education-num                        0.066930\n",
       "marital-status_Married-civ-spouse    0.052850"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e) Identify the top 5 features. Feel free to print a list.\n",
    "\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_, index = X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     17265\n",
      "           1       0.97      0.95      0.96      5527\n",
      "\n",
      "    accuracy                           0.98     22792\n",
      "   macro avg       0.98      0.97      0.97     22792\n",
      "weighted avg       0.98      0.98      0.98     22792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the model from part B, predict for the train data. Look at the classification report for the train data - is there overfitting for the RandomForest model happening?\n",
    "\n",
    "y_pred_train = rf.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Precision and Recall in the train data result is much higher than the corresponding metrics in the test data report. It is likely that there is an overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine whether overfitting is occurring in the RandomForest model, you can look at the classification report for the train data and compare it with the classification report for the test data.\n",
    "\n",
    "If the classification report for the train data shows significantly higher accuracy and F1 scores than the classification report for the test data, then there is a high likelihood that the model is overfitting. Overfitting occurs when the model becomes too complex and starts to fit to noise in the training data rather than the underlying patterns.\n",
    "\n",
    "Some indicators of overfitting in the classification report include:\n",
    "\n",
    "A much higher accuracy score on the train data compared to the test data\n",
    "A much higher F1 score on the train data compared to the test data\n",
    "A higher precision score and a lower recall score on the train data compared to the test data\n",
    "Therefore, to answer this question, you can compare the classification report for the train data with the classification report for the test data and check for significant differences in the performance metrics, especially accuracy and F1 scores. If the train data has much higher scores than the test data, then it is likely that the model is overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AdaBoost Classifier - GridSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9253193104965561"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a) Use the AdaBoostClassifier along with the GridSearchCV tool. Run the GridSearchCV using the following: n_estimators: 100, 200, 300, 400. learning_rate: 0.2,0.4,0.6,0.8,1, 1.2\n",
    "# Use 5 cross-fold and for scoring use \"roc_auc\" (this is the score that will be referenced when identifying the best parameters).\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "ada = AdaBoostClassifier()\n",
    "param_grid = {'n_estimators': [100, 200, 300, 400], 'learning_rate': [0.2,0.4,0.6,0.8,1, 1.2]}\n",
    "grid_search = GridSearchCV(ada, param_grid, cv=5, scoring='roc_auc')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# b) Print the best parameters and the best score.\n",
    "\n",
    "grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1.2, 'n_estimators': 400}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7008,  447],\n",
       "       [ 807, 1507]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the best parameters to fit a new model. {'learning_rate': 1.2, 'n_estimators': 400}\n",
    "ada_best = AdaBoostClassifier(learning_rate=1.2, n_estimators=400)\n",
    "ada_best.fit(X_train, y_train)\n",
    "\n",
    "# b) Use the best estimator from GridSearchCV to predict on test data. Use the .predict_proba() and the .predict() methods to get predicted probabilities as well as predicted classes.\n",
    "y_pred = ada_best.predict(X_test)\n",
    "y_pred_proba = ada_best.predict_proba(X_test)\n",
    "\n",
    "# c) Calculate the confusion matrix and classification report (both are in sklearn.metrics).\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92      7455\n",
      "           1       0.77      0.65      0.71      2314\n",
      "\n",
      "    accuracy                           0.87      9769\n",
      "   macro avg       0.83      0.80      0.81      9769\n",
      "weighted avg       0.87      0.87      0.87      9769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9244122180504519"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d) Calculate the AUC score.\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>capital-gain</th>\n",
       "      <td>0.3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital-loss</th>\n",
       "      <td>0.2900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours-per-week</th>\n",
       "      <td>0.0575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education-num</th>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                importance\n",
       "capital-gain        0.3300\n",
       "capital-loss        0.2900\n",
       "age                 0.1075\n",
       "hours-per-week      0.0575\n",
       "education-num       0.0100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ) Identify the top 5 features. Feel free to print a list OR to make a plot.\n",
    "\n",
    "feature_importances = pd.DataFrame(ada_best.feature_importances_, index = X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_importances.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     17265\n",
      "           1       0.78      0.66      0.71      5527\n",
      "\n",
      "    accuracy                           0.87     22792\n",
      "   macro avg       0.84      0.80      0.82     22792\n",
      "weighted avg       0.87      0.87      0.87     22792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# f) Using the model from part (b), predict for the train data. Look at the classification report for the train data - is there overfitting for the best estimator?\n",
    "\n",
    "y_pred_train = ada_best.predict(X_train)\n",
    "\n",
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no significant change in precision, recall and f1 score between the two classfication reports, there is no overfitting for the best estimator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Boosting Classifier - GridSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9283393044378571"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # a) Use GradientBoostingClassifier along with the GridSearchCV tool. Run the GridSearchCV using the following hyperparameters:n_estimators: 100,200, 300 & 400. learning_rate: choose 3 learning rates of your choice. max_depth: , 2 (you can try deeper, but remember part of the value of boosting stems from minimal complexity of trees)\n",
    "\n",
    "# Use 5 cross-fold and for scoring use \"roc_auc\" (this is the score that will be referenced when identifying the best parameters).\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "param_grid = {'n_estimators': [100, 200, 300, 400], 'learning_rate': [0.2,0.4,0.6,0.8,1, 1.2], 'max_depth': [1,2]}  \n",
    "grid_search = GridSearchCV(gb, param_grid, cv=5, scoring='roc_auc')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# b) Print the best parameters and the best score.\n",
    "\n",
    "grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 400}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7031,  424],\n",
       "       [ 814, 1500]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Use the parameters: 'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 400 to fit a Gradient Boosting Classifier model.\n",
    "gb = GradientBoostingClassifier(learning_rate=0.2, max_depth=2, n_estimators=400)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gb.predict(X_test)\n",
    "y_pred_proba = gb.predict_proba(X_test)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92      7455\n",
      "           1       0.78      0.65      0.71      2314\n",
      "\n",
      "    accuracy                           0.87      9769\n",
      "   macro avg       0.84      0.80      0.81      9769\n",
      "weighted avg       0.87      0.87      0.87      9769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9259990655543748"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d) Calculate the AUC score\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>marital-status_Married-civ-spouse</th>\n",
       "      <td>0.360032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital-gain</th>\n",
       "      <td>0.219108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education-num</th>\n",
       "      <td>0.173390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital-loss</th>\n",
       "      <td>0.066789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.062198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   importance\n",
       "marital-status_Married-civ-spouse    0.360032\n",
       "capital-gain                         0.219108\n",
       "education-num                        0.173390\n",
       "capital-loss                         0.066789\n",
       "age                                  0.062198"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e) Identify the top 5 features. Feel free to print a list OR to make a plot.\n",
    "\n",
    "feature_importances = pd.DataFrame(gb.feature_importances_, index = X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     17265\n",
      "           1       0.81      0.67      0.73      5527\n",
      "\n",
      "    accuracy                           0.88     22792\n",
      "   macro avg       0.85      0.81      0.83     22792\n",
      "weighted avg       0.88      0.88      0.88     22792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# f) Using the model from part (b), predict for the train data. Look at the classification report for the train data - is there overfitting for the best estimator?\n",
    "\n",
    "y_pred_train = gb.predict(X_train)\n",
    "\n",
    "print(classification_report(y_train, y_pred_train))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no significant change in precision, recall and f1 score between the two classfication reports, there is no overfitting for the best estimator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBoost - RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9287842250954712"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use xgboost along with the RandomizedSearchCV. use the following parameters: n_estimators: 100-1000 in increments of 50  (i.e. 100,150,200,.....1000). learning_rate: 0.1 - 1.6 in increments of 0.1. max_depth: 1, 2. gamma: 0 - 5 in increments of 0.25\n",
    "# For RandomizedSearchCV make sure to still use cv = 5 and for scoring use \"roc_auc\".\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "param_grid = {'n_estimators': [100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000], 'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6], 'max_depth': [1,2], 'gamma': [0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.25, 4.5, 4.75, 5]}\n",
    "random_search = RandomizedSearchCV(xgb, param_grid, cv=5, scoring='roc_auc')\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# b) Print the best parameters and the best score.\n",
    "random_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 800, 'max_depth': 1, 'learning_rate': 1.3, 'gamma': 0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7016,  439],\n",
       "       [ 817, 1497]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the parameters to fit a new XGboost model. {'n_estimators': 200, 'max_depth': 2, 'learning_rate': 0.8, 'gamma': 0}\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=800, max_depth=1, learning_rate=1.3, gamma=0)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "y_pred_proba = xgb.predict_proba(X_test)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92      7455\n",
      "           1       0.77      0.65      0.70      2314\n",
      "\n",
      "    accuracy                           0.87      9769\n",
      "   macro avg       0.83      0.79      0.81      9769\n",
      "weighted avg       0.87      0.87      0.87      9769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classficaiton report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9261158132894167"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the AUC score\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>marital-status_Married-civ-spouse</th>\n",
       "      <td>0.787646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education-num</th>\n",
       "      <td>0.057577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_Bachelors</th>\n",
       "      <td>0.036805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occupation_Exec-managerial</th>\n",
       "      <td>0.010889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occupation_Farming-fishing</th>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   importance\n",
       "marital-status_Married-civ-spouse    0.787646\n",
       "education-num                        0.057577\n",
       "education_Bachelors                  0.036805\n",
       "occupation_Exec-managerial           0.010889\n",
       "occupation_Farming-fishing           0.010100"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e) Identify the top 5 features. Feel free to print a list OR to make a plot.\n",
    "\n",
    "feature_importances = pd.DataFrame(xgb.feature_importances_, index = X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     17265\n",
      "           1       0.79      0.66      0.72      5527\n",
      "\n",
      "    accuracy                           0.88     22792\n",
      "   macro avg       0.85      0.80      0.82     22792\n",
      "weighted avg       0.87      0.88      0.87     22792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# f) Using the model from part (b), predict for the train data. Look at the classification report for the train data - is there overfitting for the best estimator?\n",
    "\n",
    "y_pred_train = xgb.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no significant change in precision, recall and f1 score between the two classfication reports, there is no overfitting for the best estimator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Moving into Conceptual Problems:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) The lecture notes describe the Gini Index which is the default criterion used for splitting in sklearn's version of RandomForestClassifier. How does the Gini Index work? (i.e. How is it used to build a top-performing model?). \n",
    "\n",
    "The Gini Index is calculated for each feature at each split point during the construction of a decision tree. The goal is to find the split point that minimizes the Gini Index, resulting in a \"pure\" split with homogeneous samples in each resulting child node.\n",
    "\n",
    "The Gini Index helps in building a top-performing model by creating splits that result in homogeneous child nodes, where the samples are more pure and of the same class. With using the Gini Index, the model can make more accurate predictions, as samples in each child node are more likely to belong to the same class, reducing the misclassification rate. \n",
    "\n",
    "Additionally, the Gini Index is computationally efficient to calculate, making it a popular choice for decision tree-based algorithms such as RandomForestClassifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Describe how Random Forest is different from bagging & why this difference can yield improved results.\n",
    "\n",
    "The Random Forest is different from bagging in several key ways, which can lead to improved results:\n",
    "\n",
    "Feature Randomness: In a Random Forest, each decision tree is trained on a random subset of features from the original feature set. This introduces additional randomness beyond the random sampling of data points in bagging. This feature randomness ensures that each tree in the Random Forest is trained on a different set of features, which can help to reduce overfitting and increase the diversity among the trees.\n",
    "\n",
    "Random Sampling: In bagging, each tree is trained on a bootstrap sample, which is a random sample with replacement from the original dataset. In a Random Forest, each tree is also trained on a bootstrap sample, but with an additional random sampling without replacement. This means that not all samples are included in each bootstrap sample, further increasing the diversity among the trees.\n",
    "\n",
    "Voting: In bagging, the final prediction is usually obtained by averaging or taking a majority vote of the predictions from all the base models. In a Random Forest, the final prediction is obtained by averaging or taking a majority vote of the predictions from all the decision trees in the forest. The voting process in Random Forest can help to improve the overall prediction accuracy and reduce the variance compared to bagging.\n",
    "\n",
    "Ensemble Size: Bagging typically involves building an ensemble of base models (e.g., decision trees) with a fixed size. In contrast, Random Forest allows for a flexible ensemble size, where the number of decision trees in the forest can be controlled. This flexibility provides an additional hyperparameter to optimize and can potentially yield improved results.\n",
    "\n",
    "The combination of feature randomness, random sampling, voting, and flexibility in ensemble size in Random Forest can lead to improved results compared to bagging. The diverse set of decision trees in a Random Forest can collectively make more accurate predictions and reduce overfitting, resulting in a more robust and higher-performing model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Describe the importance of the max_depth parameter in Random Forest. Do not just provide a definition, rather think through how bias-variance tradeoff might be impacted by the max_depth parameter.\n",
    "\n",
    "The max_depth parameter in Random Forest controls the maximum depth allowed for decision trees in the ensemble. It impacts the bias-variance tradeoff, where higher values can lead to overfitting (high variance) and lower values can lead to underfitting (high bias). \n",
    "\n",
    "When max_depth is set to a high value, the decision trees in the Random Forest can become overly complex and may lead to overfitting. This is because the decision trees can capture noise and outliers in the training data, resulting in poor generalization performance on unseen data. \n",
    "\n",
    "On the other hand, when max_depth is set to a low value, the decision trees are more constrained and may not capture all the underlying patterns in the data, leading to underfitting.\n",
    "\n",
    "Choosing an appropriate value for max_depth is crucial to strike a balance between bias and variance, and it is often determined through hyperparameter tuning techniques like cross-validation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d)What does the alpha parameter represent in AdaBoost? Please refer to chapter 7 of the Hands-On ML book if you are struggling.\n",
    "\n",
    "The alpha parameter in AdaBoost controls the contribution of each weak learner to the final ensemble by determining the update magnitude of sample weights during training. It is a hyperparameter that needs to be tuned carefully during the model selection process to achieve the best performance of the AdaBoost algorithm.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) In AdaBoost explain how the final predicted class is determined. Be sure to reference the alpha term in your explanation.\n",
    "\n",
    " The final predicted class in AdaBoost is determined through a weighted majority voting scheme, where each weak classifier's prediction is weighted by its corresponding alpha term, which is calculated based on its performance in terms of the weighted misclassification rate. The higher the alpha value of a weak classifier, the more influential its prediction will be in the final ensemble prediction.\n",
    "\n",
    " The general phase to determine the final predicted class in AdaBoost is as follows:\n",
    "\n",
    " 1. Training Phase: Each weak classifier is trained to minimize the weighted misclassification rate, where the weights are initialized uniformly across all training samples. After each iteration, the misclassified samples are given higher weights, while correctly classified samples are given lower weights, creating a focus on the samples that were misclassified by previous weak classifiers.\n",
    "\n",
    " 2. Alpha Calculation:\n",
    "The alpha term (Î±) is calculated for the trained weak classifier. The alpha term quantifies the performance of the weak classifier, with higher values indicating better performance. The formula for calculating alpha is:\n",
    "\n",
    "Î± = 0.5 * ln((1 - error) / error)\n",
    "\n",
    "where 'error' represents the weighted misclassification rate of the weak classifier, calculated as the sum of weights of misclassified samples divided by the sum of weights of all samples.\n",
    "\n",
    "3. Weight Update:\n",
    "The weights of the training samples are updated based on the misclassification rate of the weak classifier and the calculated alpha. \n",
    "\n",
    "4. Final Prediction:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) In Gradient Boosting, what is the role of the max_depth parameter? Why is it important to tune on this parameter?\n",
    "\n",
    "\n",
    "There are some key roles of the max_depth parameter in Gradient Boosting:\n",
    "\n",
    "1. Model Complexity: The max_depth parameter controls the maximum depth allowed for each decision tree in the ensemble. This impacts the model complexity. Deeper trees can capture more intricate interactions between features in the training data, which may be unnecessary and can result in decreased model interpretability and increased computational complexity. Shallower trees, on the other hand, tend to be simpler and more interpretable.\n",
    "\n",
    "2. Regularization: Limiting the depth of each tree using the max_depth parameter can help prevent overfitting, which occurs when the model learns to memorize the training data instead of generalizing well to unseen data. Shallow trees (low max_depth) are less likely to overfit as they capture simpler patterns in the data, while deep trees (high max_depth) can capture more complex and noisy patterns, potentially leading to overfitting.\n",
    "\n",
    "3. Avoidance of Overfitting: Gradient boosting is an iterative process where each subsequent tree corrects the residuals of the previous trees. If the trees are allowed to grow too deep (high max_depth), they can end up fitting the residuals too closely, leading to overfitting. Regularizing the tree depth with the max_depth parameter can help prevent this issue by constraining the model's ability to overfit the training data.\n",
    "\n",
    "4. Computational Efficiency: Training deeper trees requires more computational resources, such as memory and processing power, as the tree structure becomes more complex. Therefore, Limiting the depth of trees with the max_depth parameter can help manage the computational cost of training the model, making it more efficient.\n",
    "\n",
    "Therefore, hyperparameter tuning techniques can be used to determine the best value for max_depth that yields the best performance for a given problem.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g) In Part (e) of Steps 2-5 you determined the top 5 predictors across each model. Do any predictors show up in the top 5 predictors for all three models? If so, comment on if this predictor makes sense given what you are attempting to predict. (Note: If you don't have any predictors showing up across all 3 predictors, explain one that shows up in 2 of them).\n",
    "\n",
    "Some key features that are commenly important across different models are:\n",
    "1. marital-status Married-civ-spouse\n",
    "2. capital-gain\n",
    "3. education-num\n",
    "4. age\n",
    "\n",
    "These features are important across different models because they are highly correlated with the target variable. For example, the marital-status Married-civ-spouse feature is highly correlated with the target variable, as people who are married are more likely to have a higher income than those who are not married. Similarly, the capital-gain feature is highly correlated with the target variable, as people with higher capital gains are more likely to have a higher income than those with lower capital gains. The education-num and age features are also highly correlated with the target variable, as people with higher education and higher age are more likely to have a higher income than those with lower education and lower age.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### h) From the models run in steps 2-5, which performs the best based on the Classification Report? Support your reasoning with evidence from your test data and be sure to share the optimal hyperparameters found from your grid search.\n",
    "\n",
    "The Random Forest: \n",
    "                \n",
    "                precision    recall  f1-score   support\n",
    "           0       0.89      0.92      0.90      7455\n",
    "           1       0.70      0.62      0.66      2314\n",
    "\n",
    "    accuracy                           0.85      9769\n",
    "    Macro avg       0.79      0.77      0.78      9769\n",
    "    weighted avg       0.84      0.85      0.84      9769\n",
    "Adaboost: \n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "           0       0.90      0.94      0.92      7455\n",
    "           1       0.77      0.65      0.71      2314\n",
    "    accuracy                           0.87      9769\n",
    "    macro avg       0.83      0.80      0.81      9769\n",
    "    weighted avg       0.87      0.87      0.87      976\n",
    "\n",
    "Gradient Boosting: \n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      0.94      0.92      7455\n",
    "           1       0.78      0.65      0.71      2314\n",
    "\n",
    "    accuracy                           0.87      9769\n",
    "    macro avg       0.84      0.80      0.81      9769\n",
    "    weighted avg       0.87      0.87      0.87      9769\n",
    "\n",
    "XGboost: \n",
    "\n",
    "            precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      0.94      0.92      7455\n",
    "           1       0.77      0.65      0.70      2314\n",
    "\n",
    "    accuracy                           0.87      9769\n",
    "    macro avg       0.83      0.79      0.81      9769\n",
    "    weighted avg       0.87      0.87      0.87      9769\n",
    "\n",
    "#### Based on the above classification reports, The XGboost and Gradient Boost has the best performances with highest precision, recall and f1-score which indicates a good trade-off\n",
    "\n",
    "#### The optimal hyperparameters found for XGboost are:\n",
    "\n",
    "{'n_estimators': 800, 'max_depth': 1, 'learning_rate': 1.3, 'gamma': 0}\n",
    "\n",
    "#### The optimal hyperparameters found for Gradient Boosting are:\n",
    "\n",
    "{'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 400}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) For your best performing model, plot out a ROC curve using your test data. Feel free to use sklearn, matplotlib or any other method in python. Describe what the x-axis & y-axis of the ROC curve tell us about a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For your best performing model(XGboost), plot out a ROC curve using your test data. Feel free to use sklearn, matplotlib or any other method in python. Describe what the x-axis & y-axis of the ROC curve tell us about a classifier.\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:,1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x-axis is the false positive rate and the y-axis is the true positive rate. The ROC curve is a plot of the true positive rate against the false positive rate. The closer the curve is to the top left corner, the better the model is at predicting the positive class. The closer the curve is to the diagonal line, the worse the model is at predicting the positive class.\n",
    "\n",
    "From the above plot, it is clear that the XGboost model performs well in predicting the true positive results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
